{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic algorithm/strategy selection\n",
    "Here we will show you how you can perform an algorithm selection, helping you choose the best optimization algorithm for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaheuristic_designer import simple\n",
    "from metaheuristic_designer.operators import OperatorVector\n",
    "from metaheuristic_designer.selectionMethods import ParentSelection, SurvivorSelection\n",
    "from metaheuristic_designer.algorithms import AlgorithmSelection, StrategySelection\n",
    "from metaheuristic_designer.initializers import UniformVectorInitializer\n",
    "from metaheuristic_designer.strategies import HillClimb, SA, ES, GA, DE, PSO, RandomSearch\n",
    "from metaheuristic_designer.benchmarks import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we will use the Rastrigin function for which we need to find the minimum value. This is a very hard optimization problem and will allow us to compare the performance of different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rastrigin_func = Rastrigin(size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm selection\n",
    "We will first explore the AlgorithmSelection class, which launches each of the specified algorithms multiple times and gets some statistics about the evaluation.\n",
    "\n",
    "We begin by defining the algorithm we are going to use. The ```simple``` package gives us premade implementations of well known optimization algorithms with a single function call, although customization is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters common to all algorithms\n",
    "params = {\n",
    "    # Stopping condition\n",
    "    \"stop_cond\": \"neval\",\n",
    "    \"neval\": 5e4,\n",
    "    # Encoding\n",
    "    \"encoding\": \"real\",\n",
    "    # Disable verbose, the output gets very cluttered otherwise\n",
    "    \"verbose\": False,\n",
    "    # Number of times to launch each algorithm\n",
    "    \"repetitions\": 10,\n",
    "}\n",
    "\n",
    "# Define the algorithms we are going to use\n",
    "algorithms = [\n",
    "    simple.random_search(params, rastrigin_func),\n",
    "    simple.hill_climb(params, rastrigin_func),\n",
    "    simple.simulated_annealing(params, rastrigin_func),\n",
    "    simple.evolution_strategy(params, rastrigin_func),\n",
    "    simple.genetic_algorithm(params, rastrigin_func),\n",
    "    simple.differential_evolution(params, rastrigin_func),\n",
    "    simple.particle_swarm(params, rastrigin_func),\n",
    "]\n",
    "\n",
    "# Instanciate the AlgorithmSelection class\n",
    "algorithm_search = AlgorithmSelection(algorithms, params)\n",
    "\n",
    "# Launch all the algorithms and get the best overall solution\n",
    "solution, best_fitness, report = algorithm_search.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimize function should have returned the best overall solution and a report with statistics about the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best solution: {solution}\")\n",
    "print(f\"fitness: {best_fitness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.sort_values(\"fitness_avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can clearly see that the best algorithm for this task seems to be the Differential Evolution algorithm, and surprisingly, RandomSearch performs better than HillClimb, SA, ES and even GA. This could be caused by some misconfiguration of parameters, but since we just used the default implementation, we have no way of changing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Selection\n",
    "We could have specified the algorithms directly constructing them piece by piece, but unless you need to try some specific implementation of an algorithm, it is much simpler to search for the best Search Strategy. This is where the StrategySearch class is used. It is very similar to the AlgorithmSelection class, but using search strategies.\n",
    "\n",
    "This also has the advantage that you can choose a custom name for each strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer for sinlge-solution algorithms\n",
    "single_initializer = UniformVectorInitializer(rastrigin_func.vecsize, rastrigin_func.low_lim, rastrigin_func.up_lim, pop_size=1)\n",
    "\n",
    "# Initializer for population-based algorithms\n",
    "pop_initializer = UniformVectorInitializer(rastrigin_func.vecsize, rastrigin_func.low_lim, rastrigin_func.up_lim, pop_size=100)\n",
    "\n",
    "# Define strategies to be tested\n",
    "strategies = [\n",
    "    RandomSearch(pop_initializer),\n",
    "    HillClimb(\n",
    "        single_initializer,\n",
    "        OperatorVector(\"RandNoise\", {\"distrib\": \"Gauss\", \"F\": 1e-3}),\n",
    "        name=\"HillClimb\",\n",
    "    ),\n",
    "    SA(\n",
    "        single_initializer,\n",
    "        OperatorVector(\"RandNoise\", {\"distrib\": \"Gauss\", \"F\": 1e-3}),\n",
    "        params={\"iter\": 100, \"temp_init\": 1, \"alpha\": 0.995},\n",
    "        name=\"SA\",\n",
    "    ),\n",
    "    SA(\n",
    "        pop_initializer,\n",
    "        OperatorVector(\"RandNoise\", {\"distrib\": \"Gauss\", \"F\": 1e-3}),\n",
    "        params={\"iter\": 100, \"temp_init\": 1, \"alpha\": 0.995},\n",
    "        name=\"ParallelSA\",\n",
    "    ),\n",
    "    ES(\n",
    "        pop_initializer,\n",
    "        mutation_op=OperatorVector(\"RandNoise\", {\"distrib\": \"Gauss\", \"F\": 5e-3}),\n",
    "        survivor_sel=SurvivorSelection(\"(m+n)\"),\n",
    "        params={\"offspringSize\": 150},\n",
    "        name=\"ES-(μ+λ)\",\n",
    "    ),\n",
    "    ES(\n",
    "        pop_initializer,\n",
    "        mutation_op=OperatorVector(\"RandNoise\", {\"distrib\": \"Gauss\", \"F\": 5e-2}),\n",
    "        survivor_sel=SurvivorSelection(\"(m,n)\"),\n",
    "        params={\"offspringSize\": 700},\n",
    "        name=\"ES-(μ,λ)\",\n",
    "    ),\n",
    "    GA(\n",
    "        pop_initializer,\n",
    "        mutation_op=OperatorVector(\"RandNoise\", {\"distrib\": \"Gauss\", \"F\": 1e-3}),\n",
    "        cross_op=OperatorVector(\"Multipoint\"),\n",
    "        parent_sel=ParentSelection(\"Tournament\", {\"amount\": 60, \"p\": 0.1}),\n",
    "        survivor_sel=SurvivorSelection(\"Elitism\", {\"amount\": 10}),\n",
    "        params={\"pcross\": 0.9, \"pmut\": 0.1},\n",
    "        name=\"GA\",\n",
    "    ),\n",
    "    DE(\n",
    "        pop_initializer,\n",
    "        OperatorVector(\"DE/best/1\", {\"F\": 0.8, \"Cr\": 0.8}),\n",
    "        name=\"DE/best/1\",\n",
    "    ),\n",
    "    DE(\n",
    "        pop_initializer,\n",
    "        OperatorVector(\"DE/rand/1\", {\"F\": 0.8, \"Cr\": 0.8}),\n",
    "        name=\"DE/rand/1\",\n",
    "    ),\n",
    "    DE(\n",
    "        pop_initializer,\n",
    "        OperatorVector(\"DE/current-to-best/1\", {\"F\": 0.8, \"Cr\": 0.8}),\n",
    "        name=\"DE/current-to-best/1\",\n",
    "    ),\n",
    "    PSO(pop_initializer, {\"w\": 0.7, \"c1\": 1.5, \"c2\": 1.5}, name=\"PSO\"),\n",
    "]\n",
    "\n",
    "# Instanciate the StrategySelection class\n",
    "algorithm_search = StrategySelection(\n",
    "    rastrigin_func,\n",
    "    strategies,\n",
    "    algorithm_params={\n",
    "        \"stop_cond\": \"neval\",\n",
    "        \"neval\": 5e4,\n",
    "        \"verbose\": False,\n",
    "    },\n",
    "    params={\"verbose\": True, \"repetitions\": 10},\n",
    ")\n",
    "\n",
    "# Launch all the algorithms and get the best overall solution\n",
    "solution, best_fitness, report = algorithm_search.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"best solution: {solution}\")\n",
    "print(f\"fitness: {best_fitness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.sort_values(\"fitness_avg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
